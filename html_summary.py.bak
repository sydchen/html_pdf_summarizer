import asyncio
import httpx
from typing import List, Dict, Generator, Optional
from bs4 import BeautifulSoup
from transformers import AutoTokenizer
import ollama


class WebArticleSummarizer:
    def __init__(self,
                 model: str = "mistral:7b",
                 token_limit: int = 3000,
                 tokenizer_name: str = "google/gemma-2b",
                 overlap_ratio: float = 0.1):
        """
        初始化網頁文章摘要器

        Args:
            model: Ollama 模型名稱
            token_limit: Token 數量限制
            tokenizer_name: Tokenizer 模型名稱
            overlap_ratio: 切分時的重疊比例
        """
        self.model = model
        self.token_limit = token_limit
        self.overlap_ratio = overlap_ratio
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        # 根據不同任務類型調整建議值
        self.recommended_limits = {
            "short_summary": 1500,
            "long_summary": 3000,
            "detailed_analysis": 4000,
            "academic_paper": 6000
        }

    def chat_with_ollama(self, messages: List[Dict[str, str]]) -> str:
        try:
            response = ollama.chat(model=self.model, messages=messages)
            return response['message']['content']
        except Exception as e:
            print(f"Ollama 對話錯誤: {e}")
            return ""

    def chat_with_ollama_stream(self, messages: List[Dict[str, str]]) -> Generator[str, None, None]:
        try:
            client = ollama.Client()
            for part in client.chat(model=self.model, messages=messages, stream=True):
                if 'message' in part and 'content' in part['message']:
                    yield part['message']['content']
        except Exception as e:
            print(f"Ollama 串流對話錯誤: {e}")
            yield f"錯誤: {str(e)}"

    async def fetch_article_text(self, url: str) -> str:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")

                # 移除不需要的標籤
                for tag in soup(["script", "style", "nav", "header", "footer", "aside"]):
                    tag.decompose()

                # 尋找文章內容
                article = (soup.find("article") or
                          soup.find("main") or
                          soup.find("div", class_=lambda x: x and any(cls in str(x).lower() for cls in ["content", "article", "post"])) or
                          soup.find("body"))

                if not article:
                    raise ValueError("找不到文章內容區塊")

                # 提取段落文字
                paragraphs = article.find_all(["p", "h1", "h2", "h3", "h4", "h5", "h6"])
                text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

                if not text:
                    raise ValueError("無法提取文章文字內容")

                return text

        except httpx.RequestError as e:
            raise ValueError(f"網路請求錯誤: {e}")
        except Exception as e:
            raise ValueError(f"文章提取錯誤: {e}")

    def fetch_article_text_sync(self, url: str) -> str:
        return asyncio.run(self.fetch_article_text(url))

    def count_tokens(self, text: str) -> int:
        """計算文字的 token 數量"""
        try:
            return len(self.tokenizer.encode(text))
        except Exception as e:
            print(f"Token 計算錯誤: {e}")
            return len(text) // 4  # 備用估算

    def count_tokens_batch(self, texts: List[str]) -> int:
        """計算文字列表的總 token 數"""
        return sum(self.count_tokens(text) for text in texts)

    def get_recommended_token_limit(self, text_length: int, task_type: str = "long_summary") -> int:
        """根據文章長度和任務類型推薦合適的 token 限制"""
        base_limit = self.recommended_limits.get(task_type, 3000)

        if text_length < 5000:
            return min(base_limit, 1500)
        elif text_length < 15000:
            return base_limit
        else:
            return min(base_limit * 1.5, 6000)

    def split_text_by_tokens(self, text: str, max_tokens: int) -> List[str]:
        """將長文字按 token 數量切分成多個部分"""
        chunks = []
        paragraphs = text.split('\n\n')
        current_chunk = ""
        current_tokens = 0
        overlap_tokens = int(max_tokens * self.overlap_ratio)

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            para_tokens = self.count_tokens(paragraph)

            # 如果單個段落就超過限制，需要進一步切分
            if para_tokens > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_tokens = 0

                # 按句子切分段落
                sentences = [s.strip() + '.' for s in paragraph.split('.') if s.strip()]
                for sentence in sentences:
                    sent_tokens = self.count_tokens(sentence)

                    if current_tokens + sent_tokens > max_tokens:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
                        current_tokens = sent_tokens
                    else:
                        current_chunk += " " + sentence if current_chunk else sentence
                        current_tokens += sent_tokens
            else:
                if current_tokens + para_tokens > max_tokens:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                    current_tokens = para_tokens
                else:
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
                    current_tokens += para_tokens

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def split_texts_by_tokens(self, texts: List[str], max_tokens: int) -> List[List[str]]:
        """將文字列表分割成符合 token 限制的群組"""
        groups = []
        current_group = []
        current_tokens = 0

        for text in texts:
            text_tokens = self.count_tokens(text)

            if current_tokens + text_tokens > max_tokens and current_group:
                groups.append(current_group)
                current_group = [text]
                current_tokens = text_tokens
            else:
                current_group.append(text)
                current_tokens += text_tokens

        if current_group:
            groups.append(current_group)

        return groups

    def generate_summary(self, content: str, current: Optional[int] = None, total: Optional[int] = None) -> str:
        """生成單個內容的摘要"""
        if current is not None and total is not None:
            print(f"正在處理摘要 {current} / {total}")

        prompt = [{
            "role": "user",
            "content": f"Write a concise summary of the following：\n\n{content}"
        }]
        return self.chat_with_ollama(prompt)

    def reduce_summaries(self, summaries: List[str], current: Optional[int] = None, total: Optional[int] = None) -> str:
        """合併多個摘要"""
        if current is not None and total is not None:
            print(f"正在合併摘要 {current} / {total}")

        merged = "\n\n".join(summaries)
        content = f"""
        The following is a set of summaries:
        {merged}
        Take these and distill it into a final, consolidated summary
        of the main themes.
        """
        prompt = [{"role": "user", "content": content}]
        return self.chat_with_ollama(prompt)

    def reduce_summaries_stream(self, summaries: List[str]) -> Generator[str, None, None]:
        """串流方式合併摘要"""
        merged = "\n\n".join(summaries)
        content = f"""
        The following is a set of summaries:
        {merged}
        Take these and distill it into a final, consolidated summary
        of the main themes.
        """
        prompt = [{"role": "user", "content": content}]

        for chunk in self.chat_with_ollama_stream(prompt):
            yield chunk

    def summarize_texts_stream(self, texts: List[str]) -> Generator[str, None, None]:
        """串流方式摘要文字列表"""
        if not texts:
            yield "沒有文字可供摘要"
            return

        try:
            # 第一階段：對每個文字生成摘要
            summaries = []
            for i, text in enumerate(texts):
                summary = self.generate_summary(text, i + 1, len(texts))
                if summary:
                    summaries.append(summary)

            if not summaries:
                yield "無法生成摘要"
                return

            # 迭代合併直到符合 token 限制
            while self.count_tokens_batch(summaries) > self.token_limit:
                chunks = self.split_texts_by_tokens(summaries, self.token_limit)

                new_summaries = []
                for i, chunk in enumerate(chunks):
                    summary = self.reduce_summaries(chunk, i + 1, len(chunks))
                    if summary:
                        new_summaries.append(summary)

                if not new_summaries:
                    break

                summaries = new_summaries

            # 最終合併
            for chunk in self.reduce_summaries_stream(summaries):
                yield chunk

        except Exception as e:
            yield f"摘要過程中發生錯誤: {str(e)}"

    def get_summary(self, url: str) -> Generator[str, None, None]:
        """獲取網頁文章摘要（串流版本）"""
        try:
            print("正在獲取網頁內容...")
            article_text = self.fetch_article_text_sync(url)

            if not article_text:
                yield "無法獲取文章內容"
                return

            print(f"文章長度: {len(article_text)} 字元")

            # 計算文章的 token 數
            total_tokens = self.count_tokens(article_text)
            print(f"文章 token 數: {total_tokens}")

            # 動態調整 token 限制
            recommended_limit = self.get_recommended_token_limit(len(article_text))
            actual_limit = min(self.token_limit, recommended_limit)
            print(f"建議 token 限制: {recommended_limit}, 實際使用: {actual_limit}")

            # 如果文章太長，先切分再處理
            if total_tokens > actual_limit:
                print(f"文章超過 token 限制 ({actual_limit})，正在切分...")
                text_chunks = self.split_text_by_tokens(article_text, actual_limit)
                print(f"切分成 {len(text_chunks)} 個部分")
            else:
                print("文章長度適中，直接處理")
                text_chunks = [article_text]

            print("開始生成摘要...")
            for chunk in self.summarize_texts_stream(text_chunks):
                yield chunk

        except Exception as e:
            yield f"處理過程中發生錯誤: {str(e)}"


if __name__ == "__main__":
    summarizer = WebArticleSummarizer(
        model="mistral:7b",
        token_limit=3000,
        overlap_ratio=0.15
    )

    url = "https://lilianweng.github.io/posts/2023-06-23-agent/"
    for chunk in summarizer.get_summary(url):
        print(chunk, end='', flush=True)

    print("\n\n" + "="*50)
